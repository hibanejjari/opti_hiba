{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61df9e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden Section Search Results:\n",
      "Optimal x for f(x): 13.672404846220903, f(x) = -14431.482938527293\n",
      "Optimal x for g(x): 2.999979571825496, g(x) = -917.9812470947998\n",
      "\n",
      "Gradient Descent (Line Search) Results for f(x):\n",
      "Optimal x: 13.672392269875692, f(x) = -14431.482938526851\n",
      "\n",
      "Scipy.optimize Results:\n",
      "Optimal x for f(x) by scipy.optimize: 13.672398202257863, f(x) = -14431.482938543628\n",
      "Optimal x for g(x) by scipy.optimize: 2.999994039139014, g(x) = -917.9945279431523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# functions definition\n",
    "def f(x):\n",
    "    return x**4 - 16*x**3 - 45*x**2 - 20*x + 203\n",
    "\n",
    "def df(x):  # Derivative of f(x)\n",
    "    return 4*x**3 - 48*x**2 - 90*x - 20\n",
    "\n",
    "def g(x):\n",
    "    return -x**5 + 2*x**4 - 23*x**3 - 12*x**2 - 36*x\n",
    "\n",
    "# Golden section search\n",
    "def golden_section_search(func, a, b, tol=1e-5):\n",
    "    gr = (np.sqrt(5) + 1) / 2  # Golden ratio\n",
    "    c = b - (b - a) / gr\n",
    "    d = a + (b - a) / gr\n",
    "    while abs(c - d) > tol:\n",
    "        if func(c) < func(d):\n",
    "            b = d\n",
    "        else:\n",
    "            a = c\n",
    "        c = b - (b - a) / gr\n",
    "        d = a + (b - a) / gr\n",
    "    return (b + a) / 2\n",
    "\n",
    "def gradient_descent(func, dfunc, x0, lr=0.001, max_iter=1000, tol=1e-5):\n",
    "    x = x0\n",
    "    for _ in range(max_iter):\n",
    "        grad = dfunc(x)\n",
    "        # Check for potential overflow\n",
    "        if np.abs(grad) > 1e10:  # Arbitrary large value check\n",
    "            print(\"Potential overflow detected, stopping iteration.\")\n",
    "            return x\n",
    "        x_new = x - lr * grad\n",
    "        if np.abs(x_new - x) < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "\n",
    "# Apply algorithms\n",
    "opt_f_golden = golden_section_search(f, 2.5, 14)\n",
    "opt_g_golden = golden_section_search(g, 2, 3)\n",
    "opt_f_grad_desc = gradient_descent(f, df, 5)  # Initial guess in the middle of range for f(x)\n",
    "\n",
    "# scipy.optimize for solving\n",
    "opt_f_scipy = minimize_scalar(f, bounds=(2.5, 14), method='bounded')\n",
    "opt_g_scipy = minimize_scalar(g, bounds=(2, 3), method='bounded')\n",
    "\n",
    "# Results\n",
    "print(\"Golden Section Search Results:\")\n",
    "print(f\"Optimal x for f(x): {opt_f_golden}, f(x) = {f(opt_f_golden)}\")\n",
    "print(f\"Optimal x for g(x): {opt_g_golden}, g(x) = {g(opt_g_golden)}\")\n",
    "\n",
    "print(\"\\nGradient Descent (Line Search) Results for f(x):\")\n",
    "print(f\"Optimal x: {opt_f_grad_desc}, f(x) = {f(opt_f_grad_desc)}\")\n",
    "\n",
    "print(\"\\nScipy.optimize Results:\")\n",
    "print(f\"Optimal x for f(x) by scipy.optimize: {opt_f_scipy.x}, f(x) = {opt_f_scipy.fun}\")\n",
    "print(f\"Optimal x for g(x) by scipy.optimize: {opt_g_scipy.x}, g(x) = {opt_g_scipy.fun}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ac5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
